#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute") # VERY IMPORTANT STEP
pre
missing.data = predict(pre, missing.data)
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
#mark the missing values.
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
plot.new(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
#Mean value imputation method 3.
library(caret)
##################################
#####Visualizing Missing Data#####
##################################
library(VIM) #For the visualization and imputation of missing values.
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power. # missing is the data we want to predict on
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
summary(cars)
sapply(cars, sd) #Standard deviations.
cor(cars) #Correlations.
#Basic graphical EDA for cars dataset.
hist(cars$speed, xlab = "Speed in MPH", main = "Histogram of Speed")
hist(cars$dist, xlab = "Distance in Feet", main = "Histogram of Distance")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2) # Slide 15 of RML_SimpleML
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
#Calculating the residual values.
residuals = cars$dist - (beta0 + beta1*cars$speed)
#Note the sum of the residuals is 0.
sum(residuals)
#Visualizing the residuals.
segments(cars$speed, cars$dist,
cars$speed, (beta0 + beta1*cars$speed),
col = "red")
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2) # Slide 15 of RML_SimpleML
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
#Calculating the residual values.
residuals = cars$dist - (beta0 + beta1*cars$speed)
#Note the sum of the residuals is 0.
sum(residuals)
#Visualizing the residuals.
segments(cars$speed, cars$dist,
cars$speed, (beta0 + beta1*cars$speed),
col = "red")
#Calculating the residual values.
residuals = cars$dist - (beta0 + beta1*cars$speed) # Y - Prediction - Epsilon, Residuals(epsilon) = Y_true - Prediction
#Note the sum of the residuals is 0.
sum(residuals)
#Visualizing the residuals.
segments(cars$speed, cars$dist,
cars$speed, (beta0 + beta1*cars$speed),
col = "red")
text(cars$speed - .5, cars$dist, round(residuals, 2), cex = 0.5)
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
summary(model) #All the summary information for the model in question. Reports:
##############################################
#####Manual example with the cars dataset#####
##############################################
help(cars)
cars #Investigating the cars dataset.
#Basic numerical EDA for cars dataset.
summary(cars) #Five number summaries.
sapply(cars, sd) #Standard deviations.
cor(cars) #Correlations.
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2) # Slide 15 of RML_SimpleML
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
library(ggplot2)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2) # Slide 15 of RML_SimpleML
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
####################################################
#####Checking assumptions with the cars dataset#####
####################################################
#Linearity
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
#Constant Variance & Independent Errors
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
summary(model) #All the summary information for the model in question. Reports:
#Notice that the F-statistic value for the overall regression is the same as the
#square of the t-statistic value for the speed coefficient:
t.statistic = 9.464
f.statistic = 89.57
t.statistic^2
confint(model) #Creating 95% confidence intervals for the model coefficients.
####################################################
#####Checking assumptions with the cars dataset#####
####################################################
#Linearity
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
#Constant Variance & Independent Errors
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
#Visualizing another influence plot for the regression model.
library(car) #Companion to applied regression.
influencePlot(model)
#####################################
#####Predicting New Observations#####
#####################################
model$fitted.values #Returns the fitted values.
newdata = data.frame(speed = c(15, 20, 25)) #Creating a new data frame to pass
predict(model, newdata, interval = "confidence") #Construct confidence intervals
predict(model, newdata, interval = "prediction") #Construct prediction invervals
#Constructing confidence and prediction bands for the scope of our data.
newdata = data.frame(speed = 4:25)
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
#Visualizing the confidence and prediction bands.
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2) #Plotting the regression line.
lines(newdata$speed, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$speed, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$speed, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$speed, pred.band[, 3], col = "red") #Plotting the upper prediction band.
####################################
#####The Box-Cox Transformation#####
####################################
library(car)
bc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda
lambda = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value, find the x which gives us the maximum of y
lambda
dist.bc = (cars$dist^lambda - 1)/lambda #Applying the Box-Cox transformation.
model.bc = lm(dist.bc ~ cars$speed) #Creating a new regression based on the
summary(model.bc) #Assessing the output of the new model.
plot(model.bc) #Assessing the assumptions of the new model.
boxCox(model.bc) #What happens if we want to apply the Box-Cox transformation
#####################################################
#####Example using the State Information Dataset#####
#####################################################
help(state.x77)
state.x77 #Investigating the state.x77 dataset.
#Cleaning up the column names so that there are no spaces.
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
states = as.data.frame(state.x77) #Forcing the state.x77 dataset to be a dataframe.
#Cleaning up the column names so that there are no spaces.
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
#Creating a population density variable.
states[,9] = (states$Population*1000)/states$Area # new column with population desnity
colnames(states)[9] = "Density"
#Basic numerical EDA for states dataset.
summary(states)
sapply(states, sd)
cor(states)
# if using simple pure linear regression, no need to normalize.
cor(states)
# if using simple pure linear regression, no need to normalize.
cor(states)
#Basic graphical EDA for the states dataset.
plot(states)
#Creating a saturated model (a model with all variables included).
model.saturated = lm(Life.Exp ~ ., data = states)
summary(model.saturated) #Many predictor variables are not significant, yet the
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
# Your code here
library(MASS)
cats
ggplot(data = cats, aes(x = cats$Bwt, y = cats$Hwt)) + geom_point(aes(color = cats$Sex)) + labs(title = "Cat Heart Weight vs. Body Weight", x = "Heart Weight", y = "Body Weight")
# Your code here
library(ggplot2)
# Your code here
library(ggplot2)
ggplot(data = cats, aes(x = cats$Bwt, y = cats$Hwt)) + geom_point(aes(color = cats$Sex)) + labs(title = "Cat Heart Weight vs. Body Weight", x = "Heart Weight", y = "Body Weight")
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cats Dataset")
# Your code here
# Part A: regression equation
model = lm(Hwt ~ Bwt, data = cats)
summary(model)
plot(cats$Bwt, cats$Hwt, xlab = "Body Weight in kg", ylab = "Heart Weight in g", main = "Scatterplot of Cats Dataset")
abline(a = -0.3567, b = 4.0341, lty = 2)
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cats Dataset")
abline(h = 0, lty = 2)
#Normality
qqnorm(model$residuals)
qqline(model$residuals)
qqnorm(model$residuals)
qqline(model$residuals)
plot(cats$Hwt, Hwt.res, main = "Residuals vs. Heart Weight", xlab = "Heart Weight", ylab = "Residuals of Heart Weight")
ggplot(data = cats, aes(x = cats$Bwt, y = cats$Hwt)) + geom_point() + labs(title = "Cat Heart Weight vs. Body Weight", x = "Heart Weight", y = "Body Weight")
Hwt.res = resid(model)
plot(cats$Hwt, Hwt.res, main = "Residuals vs. Heart Weight", xlab = "Heart Weight", ylab = "Residuals of Heart Weight")
abline(0, 0, lty = 5)
# plot the residuals
segments(cats$Bwt,
cats$Hwt,
cats$Bwt,
model$coefficients[1] + model$coefficients[2], col = 'red')
egments(cats$Bwt,
cats$Hwt,
cats$Bwt,
model$coefficients[1] + model$coefficients[2], col = 'red')
segments(cats$Bwt,
cats$Hwt,
cats$Bwt,
model$coefficients[1] + model$coefficients[2], col = 'red')
# Your code here
newdata = data.frame(Bwt = c(2.8, 5, 10))
predict(model, newdata, interval='confidence')
predict(model, newdata, interval='prediction')
Hwt.bc = log(cats$Hwt)
model.bc = lm(Hwt.bc ~ cats$Bwt) # Creating a new regression based on the transformed variable, Hwt.bc
model.bc
shiny::runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
?selectizeInput
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
df$variety[df$variety == input$variety1]
df$variety[df$variety == 'merlot']
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
df %>% select(everything(), -X.1, -X, -description, -region_2)
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
?return
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
?break
?if
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
?observe
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
shiny::runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
df
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
View(wine_words_filtered)
View(wine_words_filtered)
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
View(wine_words_filtered)
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
shiny::runApp('NYCDSA/Projects/proj2_shiny/wine_enthusiast_reviews/myApp')
shiny::runApp('NYCDSA/Classwork/Projects/proj2_shiny/wine_ratings')
setwd("~/Desktop/NYCDSA/Classwork/Projects/proj2_shiny/wine_ratings")
df = read.csv("base_df.csv", stringsAsFactors = F)
df = df %>% select(everything(), -X)
wine_words_filtered = read.csv("wine_words_2.csv", stringsAsFactors = F)
wine_words_filtered = wine_words_filtered %>% select(everything(), -X)
runApp()
plot_ly(df, x = ~price, type = 'histogram')
table(df$price)
df %>% filter(price == 2013)
df %>% filter(price != 2013) %>% plot_ly(x = ~price, type = 'histogram')
df %>% filter(price <= 300) %>% plot_ly(x = ~price, type = 'histogram')
sd(df$price)
(df$price - mean(df$price))/sd(df$price)
plot_ly(x = ~(df$price - mean(df$price))/sd(df$price), type = 'histogram')
df %>% filter(price <= mean(price) + 3*sd(price), price >= mean(price) - 3*sd(price))
df %>% filter(price <= mean(price) + 3*sd(price), price >= mean(price) - 3*sd(price)) %>% select(unique(price))
df %>% filter(price <= mean(price) + 3*sd(price), price >= mean(price) - 3*sd(price)) %>% select(price)
plot_ly(x = ~df %>% filter(price <= mean(price) + 3*sd(price), price >= mean(price) - 3*sd(price)) %>% select(price), type = 'histogram')
temp = df %>% filter(price <= mean(price) + 3*sd(price), price >= mean(price) - 3*sd(price))
plot_ly(temp, x = ~price, type = 'histogram')
density = density(temp$price)
plot_ly(x = ~density$x, y = ~density$y, type = 'scatter', mode = 'lines', fill = 'tozeroy')
max(density$x)
max(density$y)
runApp()
runApp()
?titlefont
??titlefont
shiny::titlefont
?shiny::titlefont
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?dashboardBody
runApp()
runApp()
